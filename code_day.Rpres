code_day
========================================================
author: Ram√≥n Gallego
date: 20180827
autosize: true

What does this pipeline do?
========================================================

Once you have retrieved all your sequences from the Illumina machine, you will need to asign them to the sample they came from,  and minimize the noise and cross-contamination inherent to this platform.

This pipeline will need the follwing **input** files:

- One or more pairs of fastq files
- A metadata file, with one row per sample, containing the information needed to demultiplex the sequences
```{r, echo=FALSE}
library(tidyverse)
metadata <- read_csv("data/metadata.csv")
metadata %>% select (sample_id, Tag, pri_index_name, sec_index_seq, file1, file2)
```
- A parameters file, which will be the bridge between the pipeline and the metadata file 

What does this pipeline give you?
========================================================

In return, this pipeline will give you the composition of your samples. The **output** files are:

- ASV_Table: It is in a long format, with three columns: Hash, sample, nReads

```{r, echo=FALSE}
ASV.table <- read_csv("output_training/ASV_table.csv")

ASV.table
```

- Hash_key: In case you used hashing, the key to convert sequences to hashes

```{r, echo=F}

Hash.key <- read_csv("output_training/hash_key.csv")

Hash.key

```
- A bunch of summary files and general clutter

Getting started
========================================================





- Download the last version of the pipeline. If you use git or github, open the terminal and type 

`git clone https://github.com/ramongallego/demultiplexer_for_DADA2.git <directory>`

If not, you can just browse to  https://github.com/ramongallego/demultiplexer_for_DADA2 and use the ZIP download option

- Before we started using dada2 for clustering sequences, we used jimmy O'donnell's pipeline. You can get it at

`git clone https://github.com/jimmyodonnell/banzai.git <directory2>`



Dependencies
========================================================

Here is a list of dependencies you should have for running both pipelines

- cutadapt

- vsearch 

- swarm

- seqtk

- blast+

- python

- pandoc

- and a lot of **R** packages:
  
    - data.table
    - devtools
    - reshape2
    - vegan
    - taxize
    - tidyverse
    - stringr
    - dada2
    - Biostrings
    - digest
    - rmarkdown
    - knitr

Doing a test Run: demultiplexer_for_dada2
========================================================

Just to check that you have installed everything you need, let's do a test run

`bash <path_to_demultiplexer_for_dada2>/demultiplex_both_fastqs.sh <path_to_demultiplexer_for_dada2>/banzai_params_for_dada2.sh`

The test run takes about two minutes to run, and you should see it progress like this

```{r, out.width = '55%', fig.align = 'center'}

knitr::include_graphics("code_day-figure/Screen Shot 2018-08-27 at 5.21.22 PM.png")

```


Doing a test Run: banzai
======================================


Similar concept, just try 

`bash <path/to/dir>/banzai.sh test`

If you succeed, you should see 


