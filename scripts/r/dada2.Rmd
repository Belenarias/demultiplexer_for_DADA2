---
title: "Dada2_report"
author: "Ram√≥n Gallego"
date: "1/9/2018"
output: html_document
params:
  folder:
    value: x
  hash:
    value: y
  cont:
    value: z
  fastqs:
    value: a
  original:
    value: b

---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir=params$folder)
```

## Dada2 report

You have successfully split your libraries into a pair (or two) of fastq files per sample. Now let's import the output of demultiplex_both_fastq.sh  

First load the packages. And let's update the parameters we need
```{r loading packages, echo=FALSE ,message=FALSE}
library (devtools)
library (tidyverse)
library (stringr)
library (dada2)
library (Biostrings)
library (digest)

#fastq.folder="/Users/rgallego/fastqs_demultiplexed_for_DADA2/demultiplexed_20180108_1539"
sample.map<-read_delim(paste0(params$folder,"/sample_trans.tmp"),col_names = c("Full_Id", "fastq_header","Sample"),delim = "\t")
head (sample.map)

path1= params$fastqs
params$original[1]
Sys.setenv(READ1=params$original[1])
Sys.setenv(READ2=params$original[2])
```
Firstly, we'll find the patterns that separate our Fwd, Rev, .1 and .2 files. look at the quality of the .1 and .2 reads
```{r listing files}

F1s <- sort(list.files(path1, pattern="_Fwd.1.fastq", full.names = TRUE))
F2s <- sort(list.files(path1, pattern="_Fwd.2.fastq", full.names = TRUE))
R1s <- sort(list.files(path1, pattern="_Rev.1.fastq", full.names = TRUE))
R2s <- sort(list.files(path1, pattern="_Rev.2.fastq", full.names = TRUE))

# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq
sample.names <- str_replace(basename(F1s), "_Fwd.1.fastq","")

#Now use only those that reflect a real sample

good.sample.names<-sample.names[sample.names %in% sample.map$fastq_header]
# Introduce here the biological counterpart of the fastq file

real.sample.name <- sample.map[,3][match(good.sample.names,sample.map$fastq_header),1]

# Now subset the number of files to be looked at

F1sgood<-F1s[sample.names %in% sample.map$fastq_header]
F2sgood<-F2s[sample.names %in% sample.map$fastq_header]
R1sgood<-R1s[sample.names %in% sample.map$fastq_header]
R2sgood<-R2s[sample.names %in% sample.map$fastq_header]

```

A nice idea is to plot the Quality of the F1 reads

```{r qplot1, echo=FALSE}
plotQualityProfile(F1sgood[1:4])
```

The quality should be similar to that of the Reverse .1 reads

```{r plot2, echo=FALSE}
plotQualityProfile(R1sgood[1:4])
```

On the other hand, the quality of the .2 reads should decrease earlier
```{r qplot3, echo=FALSE}
plotQualityProfile(F2sgood[1:4])
```

## Now start with the trimming of each read based on the quality we just plotted
```{r filter and trim}
filt_path <- file.path(params$folder, "/filtered") # Place filtered files in filtered/ subdirectory
filtF1s <- file.path(filt_path, paste0(good.sample.names, "_F1_filt.fastq.gz"))
filtF2s <- file.path(filt_path, paste0(good.sample.names, "_F2_filt.fastq.gz"))
out_Fs <- filterAndTrim(F1sgood, filtF1s, F2sgood, filtF2s, truncLen=c(210,210),
                      maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
                      compress=TRUE, multithread=TRUE) # On Windows set multithread=FALSE

filtR1s <- file.path(filt_path, paste0(good.sample.names, "_R1_filt.fastq.gz"))
filtR2s <- file.path(filt_path, paste0(good.sample.names, "_R2_filt.fastq.gz"))
out_Rs <- filterAndTrim(R1sgood, filtR1s, R2sgood, filtR2s, truncLen=c(210,210),
                      maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
                      compress=TRUE, multithread=TRUE)
```


```{r as tibble}
out_Fs_tibble<-tibble(file=dimnames(out_Fs)[[1]], reads.out=out_Fs[,2])
out_Rs_tibble<-tibble(file=dimnames(out_Rs)[[1]], reads.out=out_Rs[,2])

```

Now the first crucial step: learning the error rates that would be different for fwd and rev reads

```{r learning errors, echo=T}
errF1 <- learnErrors(filtF1s, multithread=TRUE,verbose = 0)
errF2 <- learnErrors(filtF2s, multithread=TRUE,verbose = 0)
errR1 <- learnErrors(filtR1s, multithread=TRUE,verbose = 0)
errR2 <- learnErrors(filtR2s, multithread=TRUE,verbose = 0)

```

Which we can plot now to see the error rates between transitions of each pair of nt
```{r plotErrors}

plotErrors(errF1, nominalQ = T)
plotErrors(errF2, nominalQ = T)
plotErrors(errR1, nominalQ = T)
plotErrors(errR2, nominalQ = T)


```

## Now go to the dereplication step

```{r dereplication, echo=F,message=FALSE}
derepF1s <- derepFastq(filtF1s, verbose=TRUE)
derepF2s <- derepFastq(filtF2s, verbose=TRUE)
derepR1s <- derepFastq(filtR1s, verbose=TRUE)
derepR2s <- derepFastq(filtR2s, verbose=TRUE)

# Name the derep-class objects by the sample names
rownames(out_Fs) <- names(derepF1s) <- names(derepF2s) <- real.sample.name$Sample

rownames(out_Rs) <- names(derepR1s) <- names(derepR2s) <- real.sample.name$Sample

```

## And finally an inference of the sample composition

```{r dadaing, message=FALSE}
dadaF1s <- dada(derepF1s, err = errF1, multithread = TRUE)
dadaF2s <- dada(derepF2s, err = errF2, multithread = TRUE)
dadaR1s <- dada(derepR1s, err = errR1, multithread = TRUE)
dadaR2s <- dada(derepR2s, err = errR2, multithread = TRUE)

```

## We are ready now to merge reads - using the denoised reads and the derep files

```{r merging pairs}

mergersF <- mergePairs(dadaF1s, derepF1s, dadaF2s, derepF2s,verbose = 0)

#Run a for loop that adds the number of unique reads that went into each ASV

for (j in 1:length(mergersF)){

  dadaF1s[[j]]@.Data[[2]] %>% rownames_to_column(var="forward") %>% select("forward", "nunq") ->Fwd
  Fwd$forward<-as.integer(Fwd$forward)
  dadaF2s[[j]]@.Data[[2]] %>% rownames_to_column(var="reverse") %>% select("reverse", "nunq") ->Rev
  Rev$reverse<-as.integer(Rev$reverse)

  mergersF[[j]] <- left_join(mergersF[[j]],Fwd, by="forward") %>% left_join(Rev, by="reverse") %>% mutate(nunq=pmin(nunq.x,nunq.y)) %>% select(-nunq.x,-nunq.y)


}

mergersR <- mergePairs(dadaR1s, derepR1s, dadaR2s, derepR2s, verbose = 0)

for (j in 1:length(mergersR)){

  dadaR1s[[j]]@.Data[[2]] %>% rownames_to_column(var="forward") %>% select("forward", "nunq") ->Fwd
  Fwd$forward<-as.integer(Fwd$forward)
  dadaR2s[[j]]@.Data[[2]] %>% rownames_to_column(var="reverse") %>% select("reverse", "nunq") ->Rev
  Rev$reverse<-as.integer(Rev$reverse)

  mergersR[[j]] <- left_join(mergersR[[j]],Fwd, by="forward") %>% left_join(Rev, by="reverse") %>% mutate(nunq=pmin(nunq.x,nunq.y)) %>% select(-nunq.x,-nunq.y)

}

```

## Now we have to merge the Forward and Reverse Reads to make a unique object

Step 1 is to create sequence tables for each direction, seqtabF and seqtabR
```{r merging F and R (1)}

seqtabF <- makeSequenceTable(mergersF)

dim(seqtabF)

table(nchar(getSequences(seqtabF)))

seqtabR <- makeSequenceTable(mergersR)

dim(seqtabR)

table(nchar(getSequences(seqtabR)))
```


Step  2 is to reverse complement the reverse reads, and checking how many of the reads in F were present in R (if you get a 0 means we have done something wrong). Lastly, we change the reverse sequences and add their Reverse complement instead
```{r merging F and R (2)}
reversed_sequences<-as.character(reverseComplement(DNAStringSet(colnames(seqtabR))))

summary (colnames(seqtabF) %in% reversed_sequences)

summary (reversed_sequences %in% colnames(seqtabF))

colnames(seqtabR)<-reversed_sequences

```

Step 3 does the actual merging and returns another sequence Table

```{r merging F and R (3)}

seqtab.R.df=as.data.frame(seqtabR)

final.seqtab<-data.frame(row.names = rownames(seqtabF))


for (i in 1:ncol(seqtabF)) {   #for each column of the first dataframe

  current_seq<-colnames(seqtabF)[i]

  if (current_seq %in% colnames(seqtab.R.df)) { # is that column present on the second df?

    final.seqtab[,current_seq]<-seqtabF[,i] + seqtab.R.df[,current_seq] #if yes, the new df has the sum of reads

    seqtab.R.df[,current_seq]<-NULL
    #seqtab.R2.df<- seqtab.R2.df[,-current_seq] #we delete the column from the second df to speed up next search

  
  } else {        # if the column is not present, then the new df is the value of the first df

    final.seqtab[,current_seq] <- seqtabF[,i]

  }



}
# Now cbind both dataset
final.seqtab <- as.matrix (cbind(final.seqtab,seqtab.R.df))

```

## Now get rid of the chimeras

```{r RemovingChimeras, message=F}

seqtab.nochim <- removeBimeraDenovo(final.seqtab, method="consensus", multithread=TRUE)

dim(seqtab.nochim)  


```
## Now it is a good time to check whether the sequences are in the Fwd or Reverse-ReverseComplement direction, we'll use 500 sequences to make up our test


```{r, write colnames}
ifelse(length(colnames(seqtab.nochim))>500, to_write<- sample(colnames(seqtab.nochim),size = 500, replace = F), to_write<-colnames(seqtab.nochim))
#to_write<- sample(colnames(seqtab.nochim),size = 500, replace = F)
write_lines(to_write, path = "seqnames.txt")
```

```{bash}

#This looks for the sequences of interest in the .1 .2 fastqs, returns only the nt before the match (ie, the primer)

F1_found=matchesF.txt
F2_found=matchesR.txt

cat seqnames.txt | cut -c 1-120 | xargs -i sed -n 's/{}.*$//p' "${READ1}" > "${F1_found}"

cat seqnames.txt | cut -c 1-120 | xargs -i sed -n 's/{}.*$//p' "${READ2}" > "${F2_found}"

#This looks in those files for the Fwd and Rev PCR primers - we only should get the fwd primer

COIF=$(head -2 pcr_primers.fasta | tail -1 | sed 's/[^ACGT]/\[A-Z]/g')
COIR=$(head -4 pcr_primers.fasta | tail -1 | sed 's/[^ACGT]/\[A-Z]/g')

echo "${COIF}"

matchesF1=$(grep "${COIF}" "${F1_found}" --count)
matchesRRC1=$(grep "${COIR}" "${F1_found}" --count)
matchesF2=$(grep "${COIF}" "${F2_found}" --count)
matchesRRC2=$(grep "${COIR}" "${F2_found}" --count)

echo "${matchesF1}" "${matchesRRC1}" "${matchesF2}" "${matchesRRC2}"

printf "Fwd_Primer\tfile.1\t%s\nRev_Primer\tfile.1\t%s\nFwd_Primer\tfile.2\t%s\nRev_Primer\tfile.2\t%s\n" \
 "${matchesF1}" "${matchesRRC1}" "${matchesF2}" "${matchesRRC2}" >matches.txt
```

#Now hoping these objects are available for R to import

```{r shall we rc}

results_search=read_delim("matches.txt", delim = "\t", col_names = c("Primer","File", "nMatches"))
results_search
results_search %>% group_by(Primer) %>% summarise(Total=sum(nMatches)) %>% pull (Total)-> results_spread
names(results_spread) <- results_search %>% group_by(Primer) %>% summarise(Total=sum(nMatches)) %>% pull (1)

results_spread
direction=NULL

# So here is my solution: If all or 99% of the reads are Fwd - then we assume they are all Fwd and no RC is needed
# If all or 99% of the reads are Reverse Complemented - then I assumy they are all RC and we'll proceed with revcom
# IF there is more than 1% difference then we'll call it mixed and throw a warning message and don't do anything

ifelse(results_spread["Rev_Primer"]==0 | (results_spread["Fwd_Primer"] / results_spread["Rev_Primer"])>100, yes = direction<-"Fwd",
     ifelse(results_spread["Fwd_Primer"]==0 | (results_spread["Rev_Primer"] / results_spread["Fwd_Primer"])>100, yes = direction<-"Rev", no = direction<-"MIXED"))
direction

```


## IF selected, proceed with Hashing: create a hash conversion table and saving files in tidyr format

We are going to keep the info in a tidyr format and save it into a csv file
```{r tidying and writing}

seqtab.nochim.df=as.data.frame(seqtab.nochim)

if (direction == "MIXED") {
  
  print(" Seems like some of your reads are in the Fwd direction but others are Reverse Complemented - You should look into that")
}

if (direction == "Rev") {
  
  print(" Seems like your reads are in the Reverse Complemented direction - I'm going to fix that for you ")
  
  reversed_sequences<-as.character(reverseComplement(DNAStringSet(colnames(seqtab.nochim.df))))

colnames(seqtab.nochim.df)<-reversed_sequences
  
}

if (direction == "Fwd") {
  
  print ("All your reads are in the Fwd direction - Congratulations")
}


# Now decide if you want hashing or not

if (grepl ("yes", params$hash, ignore.case = TRUE)) {

  conv_file = paste0(params$folder,"/hash_key.csv")

  conv_table <- tibble( Hash = "", Sequence ="")

  hashes <- list(NULL)

  for (i in 1:ncol(seqtab.nochim.df)) {   #for each column of the dataframe

    current_seq <-colnames(seqtab.nochim.df)[i]

    current_hash <- digest(current_seq,algo = "sha1",serialize = F,skip = "auto")

    hashes[[i]] = current_hash

    conv_table [i,]<-c(current_hash, current_seq)

    colnames(seqtab.nochim.df)[i] <- current_hash

  }

  write_csv(conv_table, conv_file)


seqtab.nochim.df$sample=rownames(seqtab.nochim.df)

 gather(seqtab.nochim.df, key=Hash, value = nReads, -sample) %>% 
  filter(nReads > 0) -> current_asv

write_csv(current_asv, paste0(params$folder,"/ASV_table.csv"))
} else { #What do we do if you don't want hashes: two things - Change the header of the ASV table, write only one file
  
  seqtab.nochim.df$sample=rownames(seqtab.nochim.df)

  gather(seqtab.nochim.df, key=Sequence, value = nReads, -sample) %>% 
   filter(nReads > 0) -> current_asv
  write_csv(current_asv, paste0(params$folder,"/ASV_table.csv"))
}


```
## Merge with a previous analysis if selected

If we have chosen to merge our dataset with a previous run, we need to gather that info, and merge properly both datasets.
```{r merging with previous datasets}

if (grepl ("yes", params$cont[1], ignore.case = TRUE)) {

old_hash = read_csv(params$cont[2])

old_asv = read_csv(params$cont[3])

all_hash = union_all(old_hash, conv_table)

new.hashes = setdiff(conv_table,old_hash)

reads.new.hashes <- left_join(new.hashes, current_asv, by = c("Hash" = "Sequence"))

all_asv = bind_rows (old_asv, current_asv)

merging.output = tibble(Date = Sys.Date(), Old_db = params$cont[2], Old_ASV = params$cont[3],
                        Input_folder= params$folder, Nseq_old_db = nrow(old_hash), Nseq_new_input = nrow(conv_table),
                        Nseq_new_db = nrow(all_hash), N_newseq = nrow(new.hashes), Nreads_newseq = sum(reads.new.hashes$nReads) )
ifelse(file.exists(params$cont[4]),
       write_csv(merging.output, params$cont[4], append = T),
       write_csv(merging.output, params$cont[4]) )

write_csv(all_hash, paste0(params$folder, "/Hash_db_",Sys.Date(),".csv"))
write_csv(all_asv, paste0(params$folder, "/ASV_all_",Sys.Date(),".csv"))

}

```
##Track the fate of all reads
```{r output_summary}

getN <- function(x) sum(getUniques(x))
track <- as.data.frame(cbind(out_Fs, out_Rs,
                             sapply(dadaF1s, getN), sapply(dadaR1s, getN),
                             sapply(mergersF, getN),sapply(mergersR, getN),
                             rowSums(seqtabF),rowSums(seqtabR),
                             rowSums(final.seqtab), rowSums(seqtab.nochim)))
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input_F", "input_R", "filtered_F","filtered_R",
                     "denoised_F", "denoised_R",
                     "merged_F", "merged_R",
                     "tabled_F", "tabled_R",
                     "tabled_together", "nonchim")

write_csv(track, paste0(params$folder,"/dada2_summary.csv"))

```
